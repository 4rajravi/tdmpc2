# ==========================================================
# Training Configuration
# Controls learning, optimization, and replay behavior
# ==========================================================

# ----------------------------------------------------------
# Total environment interaction
# ----------------------------------------------------------
# Number of environment steps collected during training
steps: 1_000

# ----------------------------------------------------------
# Replay buffer
# ----------------------------------------------------------
buffer_size: 1_000        # Maximum replay capacity 
batch_size: 128              # Samples per gradient update 
seed_steps: 500            # Initial random exploration steps 

# -------a---------------------------------------------------
# Optimization
# ----------------------------------------------------------
lr: 3e-4                     # Base learning rate 
grad_clip_norm: 20.0         # Global gradient norm clipping 

# ----------------------------------------------------------
# Discounting & target networks
# ----------------------------------------------------------
discount: 0.99 
vmin: -10.0                  # Distributional value support 
vmax: 10.0 
tau: 0.01                    # Target network EMA rate 

# ----------------------------------------------------------
# TD-MPC2 loss weights
# ----------------------------------------------------------
reward_coef: 0.1             # Reward prediction loss
value_coef: 0.1              # Value prediction loss
consistency_coef: 20.0       # Latent dynamics consistency loss
termination_coef: 1.0        # Episode termination prediction

# ----------------------------------------------------------
# Temporal weighting (Î»)
# ----------------------------------------------------------
# Controls multi-step rollout loss decay
rho: 0.5 

# ----------------------------------------------------------
# Training schedule
# ----------------------------------------------------------
update_every: 1              # Env steps per update cycle 
updates_per_step: 1          # Gradient updates per env step 
