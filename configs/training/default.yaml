# ==========================================================
# Training Configuration
# Controls learning, optimization, and replay behavior
# ==========================================================

# ----------------------------------------------------------
# Total environment interaction
# ----------------------------------------------------------
# Number of environment steps collected during training
steps: 10_000

# ----------------------------------------------------------
# Replay buffer
# ----------------------------------------------------------
buffer_size: 10_000        # Maximum replay capacity #used
batch_size: 128              # Samples per gradient update #used
seed_steps: 5_000            # Initial random exploration steps #used

# ----------------------------------------------------------
# Optimization
# ----------------------------------------------------------
lr: 3e-4                     # Base learning rate #used
enc_lr_scale: 0.3            # Encoder LR multiplier
grad_clip_norm: 20.0         # Global gradient norm clipping #used

# ----------------------------------------------------------
# Discounting & target networks
# ----------------------------------------------------------
discount: 0.99 #used
vmin: -10.0                  # Distributional value support #used
vmax: 10.0 #used
tau: 0.01                    # Target network EMA rate #used

# ----------------------------------------------------------
# TD-MPC2 loss weights
# ----------------------------------------------------------
reward_coef: 0.1             # Reward prediction loss
value_coef: 0.1              # Value prediction loss
consistency_coef: 20.0       # Latent dynamics consistency loss
termination_coef: 1.0        # Episode termination prediction

# ----------------------------------------------------------
# Temporal weighting (Î»)
# ----------------------------------------------------------
# Controls multi-step rollout loss decay
rho: 0.5 #used

# ----------------------------------------------------------
# Training schedule
# ----------------------------------------------------------
update_every: 1              # Env steps per update cycle #used
updates_per_step: 1          # Gradient updates per env step #used

# ----------------------------------------------------------
# Performance
# ----------------------------------------------------------
compile: false                # torch.compile (PyTorch 2.x)

# ----------------------------------------------------------
# Logging
# ----------------------------------------------------------
log_interval: 100             # Training metrics logging interval
